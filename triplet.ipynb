{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from pyramda import compose, curry\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "from visuazlizers.nb_graph_visualizer import show_graph, rename_nodes\n",
    "from utils.curried_functions import tf_add, tf_cast, tf_multiply, filter_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_to_labels = compose(\n",
    "    list,\n",
    "    range,\n",
    "    len,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_batch(path, dirs, labels, num_per_class, image_shape):\n",
    "    \"\"\"\n",
    "    Loads a random batch of images\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    - path: string\n",
    "        Path to the image source directory on disk. \n",
    "        Source directory should be divided into directories, one directory per class.\n",
    "    - dirs: [string]\n",
    "        List of directories contained in the path (classes).\n",
    "    - labels: [int]\n",
    "        Class labels. Should correspond to classes.\n",
    "    - num_per_class: int\n",
    "        Number of images randomly chosen from each class\n",
    "    - image_shape: tuple (H,W,C)\n",
    "        H - image height\n",
    "        W - image width\n",
    "        C - number of channels\n",
    "      \n",
    "    Returns:\n",
    "    --------\n",
    "    - samples: ndarray (N, H, W, C)\n",
    "        Numpy array of randomly chosen images resized according to model's input shape.\n",
    "        N - number of samples\n",
    "        H - height\n",
    "        W - width\n",
    "        C - number of channels\n",
    "    - batch_labels: [int]\n",
    "        Sample labels.\n",
    "    \"\"\"\n",
    "    \n",
    "    samples = np.zeros((num_per_class * len(dirs), *image_shape))\n",
    "    batch_labels = np.ones(num_per_class * len(dirs)).astype(int)\n",
    "    \n",
    "    for i, dir_name in enumerate(tqdm(dirs)):\n",
    "        dir_path = os.path.join(path, dir_name)\n",
    "        filenames = os.listdir(dir_path)\n",
    "        filenames = np.random.choice(filenames, num_per_class)\n",
    "        \n",
    "        batch = np.zeros((num_per_class, *image_shape))\n",
    "\n",
    "        for j, filename in enumerate(filenames):\n",
    "            img = Image.open(os.path.join(dir_path, filename))\n",
    "            img = img.resize((image_shape[1], image_shape[0]))\n",
    "            img = np.array(img)\n",
    "            batch[j,:,:,:] = img\n",
    "        \n",
    "        samples[i*num_per_class: i*num_per_class + num_per_class, :, :, :] = batch\n",
    "        batch_labels[i*num_per_class: i*num_per_class + num_per_class] = batch_labels[i*num_per_class: i*num_per_class + num_per_class] * labels[i]\n",
    "    \n",
    "    return samples, batch_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.deep_sort_cnn.freeze_model import _preprocess, _network_factory\n",
    "\n",
    "def create_deep_sort_conv_graph():\n",
    "    input_var = tf.placeholder(tf.uint8, (None, 128, 64, 3), name=\"images\")\n",
    "    image_var = tf.map_fn(\n",
    "        lambda x: _preprocess(x), tf.cast(input_var, tf.float32),\n",
    "        back_prop=False\n",
    "    )\n",
    "\n",
    "    factory_fn = _network_factory()\n",
    "    features, _ = factory_fn(image_var, reuse=None)\n",
    "    features = tf.identity(features, name=\"features\")\n",
    "    \n",
    "    return input_var, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(checkpoint_filename, input_name, output_name, **kwargs):\n",
    "    \"\"\"\n",
    "    Load a model from\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    - checkpoint_filename: string\n",
    "        Path to the checkpoint on disk.\n",
    "    - input_name: string\n",
    "        Name of the input variable in the graph.\n",
    "    - output_name: string\n",
    "        Name of the output variable in the graph.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    - inputs: Tensor (N, H, W, C)\n",
    "        Images\n",
    "        N - number of samples (None)\n",
    "        H - image height\n",
    "        W - image width\n",
    "        C - number of channels\n",
    "    - outputs: Tensor (N, E)\n",
    "        Image embeddings\n",
    "        N - number of samples (None)\n",
    "        E - embedding size\n",
    "    \"\"\"\n",
    "    graph_creator = kwargs.pop('graph_creator', None)\n",
    "    \n",
    "    inputs, outputs = None, None\n",
    "    if graph_creator != None:\n",
    "        inputs, outputs = graph_creator()\n",
    "    \n",
    "    with tf.gfile.GFile(checkpoint_filename, \"rb\") as file_handle:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(file_handle.read())\n",
    "    \n",
    "    tf.import_graph_def(graph_def, name=\"net\")\n",
    "    \n",
    "    if graph_creator == None:\n",
    "        inputs = tf.get_default_graph().get_tensor_by_name(\"net/%s:0\" % input_name)\n",
    "        outputs = tf.get_default_graph().get_tensor_by_name(\"net/%s:0\" % output_name)\n",
    "    \n",
    "    return inputs, outputs, graph_def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, outputs, graph_def = load_model('./models/deep_sort_cnn/mars-small128.pb', input_name=\"images\", output_name=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_def = rename_nodes(graph_def, lambda s:\"/\".join(s.split('_',1)))\n",
    "show_graph(tmp_def)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positive_mask(labels):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    -----------\n",
    "    - labels: [int]\n",
    "        List of labels of size N (number of samples).\n",
    "    Returns:\n",
    "    ----------\n",
    "    - positive_mask: Tensor (N, N)\n",
    "        A square martix with True for all positive samples and False for all negative samples.\n",
    "    \"\"\"\n",
    "    return tf.equal(tf.expand_dims(labels, 0), tf.expand_dims(labels, 1))\n",
    "\n",
    "get_not_anchor_mask = compose(\n",
    "    tf.logical_not,\n",
    "    tf_cast(dtype = tf.bool),\n",
    "    tf.eye,\n",
    "    lambda shape: shape[0],\n",
    "    tf.shape,\n",
    ")\n",
    "\n",
    "get_not_anchor_mask.__doc__ = \"\"\"\n",
    "    Parameters:\n",
    "    -----------\n",
    "    - labels: [int]\n",
    "        List of labels of size N (number of samples).\n",
    "    Returns:\n",
    "    ----------\n",
    "    - not_anchor_mask: Tensor (N, N)\n",
    "        A square martix with False for all anchors and True for other samples, like\n",
    "        [[0 1 1]\n",
    "         [1 0 1]\n",
    "         [1 1 0]]\n",
    "\"\"\"\n",
    "\n",
    "def get_anchor_positive_mask(labels):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    -----------\n",
    "    - labels: [int]\n",
    "        List of labels of size N (number of samples).\n",
    "    Returns:\n",
    "    ----------\n",
    "    - anchor_positive_mask: Tensor (N, N)\n",
    "        A square martix with ones for all positive samples, except anchors on main diagonal, \n",
    "        and zeros for all other samples.\n",
    "    \"\"\"\n",
    "    return tf.to_float(\n",
    "        tf.logical_and(\n",
    "            get_not_anchor_mask(labels),\n",
    "            get_positive_mask(labels),\n",
    "        )\n",
    "    )\n",
    "\n",
    "get_negative_mask = compose(\n",
    "    tf_add(1.),\n",
    "    tf_multiply(np.finfo(np.float32).max),\n",
    "    tf.to_float,\n",
    "    get_positive_mask,\n",
    ")\n",
    "\n",
    "get_negative_mask.__doc__ = \"\"\"\n",
    "    Parameters:\n",
    "    -----------\n",
    "    - labels: [int]\n",
    "        List of labels of size N (number of samples).\n",
    "    Returns:\n",
    "    ----------\n",
    "    - positive_mask: Tensor (N, N)\n",
    "        A square martix with ones for all negative samples and infinity for all positive samples.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_distance(embeddings):\n",
    "    \"\"\"\n",
    "    Compute cosine distance matrix\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    - embeddings: Tensor(N, E)\n",
    "        Image embeddings, outputs of the convolutional network.\n",
    "        N - number of samples (None)\n",
    "        E - embedding size\n",
    "    \"\"\"\n",
    "    \n",
    "    normalized_embeddings = tf.divide(\n",
    "        embeddings,\n",
    "        tf.norm(embeddings),\n",
    "    )\n",
    "    \n",
    "    return tf.subtract(\n",
    "        1.,\n",
    "        tf.matmul(normalized_embeddings, tf.transpose(normalized_embeddings))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(model, metric, masks, margin):\n",
    "    \"\"\"\n",
    "    Compute triplet loss\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    - model: tuple\n",
    "        Model input tensor and model output tensor\n",
    "    - metric: function\n",
    "        Should take output tesor as a parameter and compute distance matrix\n",
    "    - masks: tuple\n",
    "        Contains two matrices: \n",
    "            a square martix with ones for all positive samples, except anchors on main diagonal, \n",
    "            and zeros for all other samples;\n",
    "            a square martix with ones for all negative samples and infinity \n",
    "            for all positive samples.\n",
    "    - margin: float\n",
    "        Minimum margin between positive and negative distance.\n",
    "    \"\"\"\n",
    "    inputs, outputs = model\n",
    "    anchor_positive_mask, negative_mask = masks\n",
    "\n",
    "    distances = metric(outputs)\n",
    "    positive_distances = tf.multiply(anchor_positive_mask, distances)\n",
    "    negative_distances = tf.multiply(negative_mask, distances)\n",
    "\n",
    "    loss = tf.expand_dims(positive_distances, 2) - tf.expand_dims(negative_distances, 1) + margin\n",
    "    loss = tf.maximum(loss, 0.)\n",
    "    \n",
    "    num_triplets = compose(\n",
    "        tf.reduce_sum,\n",
    "        tf.to_float,\n",
    "    )(tf.greater(loss, 0.))\n",
    "    \n",
    "    loss = tf.reduce_sum(loss) / (num_triplets + 1e-16)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    session,\n",
    "    model, \n",
    "    source_path, \n",
    "    dirs, \n",
    "    train_labels, \n",
    "    metric, \n",
    "    optimizer,\n",
    "    margin=0.2, \n",
    "    num_per_class=5, \n",
    "    num_iter=1000, \n",
    "):\n",
    "    train_dirs, val_dirs = dirs\n",
    "    \n",
    "    inputs, outputs = model\n",
    "    labels = tf.placeholder(name='labels', shape=(len(train_labels) * num_per_class), dtype=tf.int8)\n",
    "    anchor_positive_mask = get_anchor_positive_mask(labels)\n",
    "    negetive_mask = get_negative_mask(labels)\n",
    "    \n",
    "    loss = compute_loss(\n",
    "        model=(inputs, outputs), \n",
    "        metric=metric, \n",
    "        masks=(anchor_positive_mask, negetive_mask), \n",
    "        margin=margin,\n",
    "    )\n",
    "    \n",
    "    train_step = optimizer.minimize(loss)\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for i in range(num_iter):\n",
    "        samples, batch_lables = load_batch(source_path, train_dirs, train_labels, num_per_class, image_shape=(128, 64, 3))\n",
    "        batch_outputs, batch_loss, _ = session.run([outputs, loss, train_step], {\n",
    "            inputs: samples,\n",
    "            labels: batch_lables,\n",
    "        })\n",
    "        print(batch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "source_path = '../input/mars/bbox_train/'\n",
    "dirs = compose(\n",
    "    filter_list(['.DS_Store'], False),\n",
    "    os.listdir,\n",
    ")(source_path)\n",
    "\n",
    "train_dirs, val_dirs = train_test_split(dirs, test_size=0.2)\n",
    "train_labels = classes_to_labels(train_dirs)\n",
    "\n",
    "inputs, outputs, _ = load_model(\n",
    "    './models/deep_sort_cnn/mars-small128.pb', \n",
    "    input_name=\"images\", \n",
    "    output_name=\"features\", \n",
    "    graph_creator=create_deep_sort_conv_graph,\n",
    ")\n",
    "\n",
    "session = tf.Session()\n",
    "\n",
    "train(\n",
    "    session=session,\n",
    "    model=[inputs, outputs],\n",
    "    source_path=source_path,\n",
    "    dirs=(train_dirs[0:5], val_dirs),\n",
    "    train_labels=train_labels[0:5],\n",
    "    metric=cosine_distance,\n",
    "    optimizer=tf.train.AdamOptimizer(learning_rate=0.00001),\n",
    "    num_iter=2,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
